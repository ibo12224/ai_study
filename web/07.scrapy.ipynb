{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294f9205",
   "metadata": {},
   "source": [
    "# 🕷️ Scrapy 사용법\n",
    "\n",
    "Scrapy는 **웹 크롤링(Web Crawling)** 과 **웹 스크레이핑(Web Scraping)** 을 빠르고 효율적으로 할 수 있게 해주는 파이썬 프레임워크입니다.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##  1. Scrapy 설치하기\n",
    "\n",
    "### **1) pip로 설치**\n",
    "Scrapy는 pip 한 줄로 설치할 수 있습니다.\n",
    "\n",
    "```bash\n",
    "pip install scrapy\n",
    "```\n",
    "\n",
    "##  2. Scrapy 프로젝트 만들기\n",
    "```bash\n",
    "scrapy startproject mycrawler\n",
    "\n",
    "\n",
    "mycrawler/\n",
    " ├── mycrawler/\n",
    " │    ├── spiders/       ← 크롤러 파일 저장하는 곳\n",
    " │    ├── settings.py\n",
    " │    ├── items.py\n",
    " │    └── middlewares.py\n",
    " └── scrapy.cfg\n",
    "\n",
    "\n",
    "예시 사이트: https://quotes.toscrape.com\n",
    " (연습용 크롤링 사이트)\n",
    "\n",
    "아래 명령으로 템플릿 크롤러 생성:\n",
    "\n",
    "cd mycrawler\n",
    "scrapy genspider quotes quotes.toscrape.com\n",
    "\n",
    "생성된 파일 경로:\n",
    "mycrawler/spiders/quotes.py\n",
    "\n",
    "프로젝트 폴더에서 실행:\n",
    "scrapy crawl quotes\n",
    "\n",
    "결과를 JSON 파일로 저장\n",
    "scrapy crawl quotes -o quotes.json\n",
    "\n",
    "{\n",
    "  \"text\": \"“The world as we have created it is a process of our thinking.”\",\n",
    "  \"author\": \"Albert Einstein\",\n",
    "  \"tags\": [\"change\", \"deep-thoughts\", \"thinking\", \"world\"]\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5103ba94",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "##  1. `spiders/` 폴더  \n",
    "크롤러 파일을 저장하는 폴더입니다.  \n",
    "우리가 직접 만드는 `quotes.py`, `news.py` 같은 스파이더들이 모두 여기에 들어갑니다.\n",
    "\n",
    "- 한 스파이더가 한 사이트(혹은 특정 기능)를 담당\n",
    "- 예:  \n",
    "\n",
    "```bash\n",
    "spiders/\n",
    "├── quotes.py\n",
    "├── products.py\n",
    "└── blog.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##  2. `settings.py`  \n",
    "Scrapy 전체 동작 환경 설정 파일입니다.\n",
    "\n",
    "여기서 다음 설정을 합니다:\n",
    "\n",
    "| 설정 항목 | 설명 |\n",
    "|----------|------|\n",
    "| `USER_AGENT` | 크롤러의 HTTP User-Agent 변경 |\n",
    "| `DOWNLOAD_DELAY` | 요청 간 딜레이 설정(방문 속도 조절) |\n",
    "| `ROBOTSTXT_OBEY` | robots.txt 준수 여부 |\n",
    "| `ITEM_PIPELINES` | 파이프라인 활성화 |\n",
    "| `DOWNLOADER_MIDDLEWARES` | 다운로더 미들웨어 등록 |\n",
    "| `CONCURRENT_REQUESTS` | 동시에 보낼 요청 수 제한 |\n",
    "\n",
    "예시:\n",
    "\n",
    "```python\n",
    "\n",
    "# robots.txt 때문에 차단되는 경우(설정무시)\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# 어플리케이션 크롤링 차단의 경우\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0 Safari/537.36\"\n",
    "\n",
    "# 짧은 시간에 너무 많은 요청을 보내면 403/429 오류\n",
    "DOWNLOAD_DELAY = 1   # 1초 딜레이\n",
    "\n",
    "# 요청수 제한의 경우\n",
    "CONCURRENT_REQUESTS = 5\n",
    "\n",
    "# Header가 부족하거나 특정 Header를 요구하는 경우\n",
    "# (일부 서버는 User-Agent 이외에도 Referer, Accept-Language 등을 확인\n",
    "DEFAULT_REQUEST_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Accept-Language\": \"ko-KR,ko;q=0.9\",\n",
    "    \"Referer\": \"https://google.com\"\n",
    "}\n",
    "\n",
    "# 과도한 크롤링으로 IP를 차단한 경우\n",
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'mycrawler.middlewares.ProxyMiddleware': 350,\n",
    "}\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "## 3. items.py\n",
    "\n",
    "크롤링한 데이터를 구조화하는 모델(데이터 스키마)을 정의하는 파일입니다.\n",
    "\n",
    "예시:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class QuoteItem(scrapy.Item):\n",
    "    text = scrapy.Field()\n",
    "    author = scrapy.Field()\n",
    "    tags = scrapy.Field()\n",
    "```\n",
    "\n",
    "효과\n",
    "\n",
    "추출 데이터의 구조를 명확히 하고\n",
    "\n",
    "pipeline 등에서 데이터를 쉽게 처리 가능\n",
    "\n",
    "\n",
    "## 4. middlewares.py\n",
    "\n",
    "- 다운로더/스파이더 미들웨어를 설정하는 파일입니다.\n",
    "\n",
    "미들웨어란?\n",
    "Scrapy에서 요청(request) 과 응답(response) 사이를 가로채서 조작하는 중간 처리기입니다.\n",
    "\n",
    "예시로 할 수 있는 작업:\n",
    "\n",
    "- User-Agent 랜덤 변경\n",
    "- Proxy 설정\n",
    "- 요청/응답 로깅\n",
    "- 쿠키 자동 처리\n",
    "\n",
    "사용 예:\n",
    "```yaml\n",
    "\n",
    "\n",
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'mycrawler.middlewares.MyCrawlerDownloaderMiddleware': 543,\n",
    "}\n",
    "\n",
    "과도한 크롤링으로 IP를 차단한 경우\n",
    "class ProxyMiddleware:\n",
    "    def process_request(self, request, spider):\n",
    "        request.meta['proxy'] = \"http://YOUR_PROXY_IP:PORT\"\n",
    "\n",
    "```\n",
    "\n",
    "## 5. scrapy.cfg\n",
    "\n",
    "프로젝트 최상단에 있으며 Scrapy 실행 설정을 관리합니다.\n",
    "CLI에서 scrapy crawl 실행할 때 이 파일을 보고 어떤 프로젝트인지 인식합니다.\n",
    "내용 예:\n",
    "```yaml\n",
    "[settings]\n",
    "default = mycrawler.settings\n",
    "```\n",
    "\n",
    "| 파일/폴더              | 역할                          |\n",
    "| ------------------ | --------------------------- |\n",
    "| **spiders/**       | 크롤러(py 파일) 저장               |\n",
    "| **settings.py**    | 프로젝트 설정(속도, 파이프라인, 미들웨어 등)  |\n",
    "| **items.py**       | 크롤링 데이터 스키마 정의              |\n",
    "| **middlewares.py** | 요청/응답을 중간에서 처리(UA, Proxy 등) |\n",
    "| **scrapy.cfg**     | Scrapy 실행 설정(CLI에서 인식용)     |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273fc5d1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| 문제 원인         | 해결 설정                            | 위치                           |\n",
    "| ------------- | -------------------------------- | ---------------------------- |\n",
    "| User-Agent 차단 | `USER_AGENT`                     | settings.py                  |\n",
    "| 빠른 요청 차단      | `DOWNLOAD_DELAY`                 | settings.py                  |\n",
    "| robots.txt 차단 | `ROBOTSTXT_OBEY = False`         | settings.py                  |\n",
    "| Header 부족     | `DEFAULT_REQUEST_HEADERS`        | settings.py                  |\n",
    "| IP 차단(Proxy)  | ProxyMiddleware                  | middlewares.py + settings.py |\n",
    "| 로그인 필요        | `start_requests` + `FormRequest` | spider 파일                    |\n",
    "| JS 렌더링 필요     | Playwright/Selenium              | settings.py + 설치             |\n",
    "\n",
    "\n",
    "\n",
    "#### 로그인 필요한 사이트\n",
    "\n",
    "로그인 없이 페이지를 막아두는 경우\n",
    "\n",
    "✔ 해결 방법\n",
    "\n",
    "```python\n",
    "스파이더에서 start_requests() 재정의 + 쿠키 유지\n",
    "\n",
    "def start_requests(self):\n",
    "    login_url = \"https://example.com/login\"\n",
    "    data = {\"id\": \"user\", \"pw\": \"1234\"}\n",
    "    yield scrapy.FormRequest(url=login_url, formdata=data, callback=self.after_login)\n",
    "\n",
    "def after_login(self, response):\n",
    "    yield scrapy.Request(\"https://example.com/protected\")\n",
    "```\n",
    "#### JavaScript 렌더링 때문에 빈 페이지만 보이는 경우\n",
    "\n",
    "Scrapy는 기본적으로 JS 실행을 못함 → 데이터가 안 보임.\n",
    "\n",
    "✔ 해결 방법\n",
    "```python\n",
    "Scrapy + Selenium\n",
    "\n",
    "또는 ScrapingBee / Zyte API 사용\n",
    "\n",
    "또는 scrapy-playwright 플러그인\n",
    "\n",
    "scrapy-playwright 예시 (settings.py)\n",
    "PLAYWRIGHT_BROWSER_TYPE = \"chromium\"\n",
    "DOWNLOAD_HANDLERS = {\n",
    "    \"http\": \"scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler\",\n",
    "    \"https\": \"scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567eb509",
   "metadata": {},
   "source": [
    "## quotes.py 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb36daab",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    allowed_domains = [\"quotes.toscrape.com\"]\n",
    "    start_urls = [\"https://quotes.toscrape.com\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # 각 quote 블록 반복\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                \"text\": quote.css(\"span.text::text\").get(),\n",
    "                \"author\": quote.css(\"small.author::text\").get(),\n",
    "                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n",
    "            }\n",
    "\n",
    "        # 다음 페이지 있는 경우 계속 이동\n",
    "        next_page = response.css(\"li.next a::attr(href)\").get()\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, callback=self.parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fe77c8",
   "metadata": {},
   "source": [
    "#  Proxy(프록시)의 역할\n",
    "\n",
    "프록시는 **\"내 대신 인터넷에 요청을 보내주는 중간 서버\"**\n",
    "\n",
    "---\n",
    "\n",
    "#  1.  비유: “심부름꾼”\n",
    "\n",
    "예를 들어 너가 어떤 가게에서 정보를 얻고 싶은데  \n",
    "가게에서 너를 알아보고 “너는 안 돼!” 하고 막아버린다고 해보자.\n",
    "\n",
    "그러면 **다른 사람(프록시)**에게 대신 가게에 가서 물어봐달라고 하는 거야.\n",
    "\n",
    "- 가게 입장 → “어? 이 사람은 처음 보네? 들어오세요!”\n",
    "- 사실은 → 그 사람이 네가 부탁한 *프록시*일 뿐\n",
    "\n",
    "즉, 프록시는 **네 IP 주소를 숨기고 대신 요청을 보내주는 역할**을 해.\n",
    "\n",
    "---\n",
    "\n",
    "#  2. 기술적으로 말하면\n",
    "\n",
    "프록시는:\n",
    "\n",
    "- 네 컴퓨터(크롤러)와 서버(웹사이트) 사이에 있는 **중간 서버**\n",
    "- 웹사이트는 프록시 IP를 진짜 접속자라고 생각\n",
    "\n",
    "---\n",
    "\n",
    "#  3. 프록시를 이용한 차단 회피\n",
    "\n",
    "웹사이트는 크롤링이 너무 많으면  \n",
    "> “이 IP 너무 많은 요청 보냈네? 차단!”\n",
    "\n",
    "하고 내 IP를 막아버릴 수 있음.\n",
    "\n",
    "하지만 프록시 IP를 사용하면:\n",
    "\n",
    "- 내 IP는 숨겨지고\n",
    "- 서버는 새로운 IP라고 생각해서 차단을 안 함\n",
    "- 여러 개의 프록시를 돌리면 더 안전함\n",
    "\n",
    "---\n",
    "\n",
    "#  4. Scrapy에서 프록시를 쓰는 이유 정리\n",
    "\n",
    "| 문제 | 프록시 사용 효과 |\n",
    "|------|-------------------|\n",
    "| 내 IP가 차단됨 | 다른 IP로 접속 가능 |\n",
    "| 요청이 너무 많음 | 여러 프록시로 분산 가능 |\n",
    "| 특정 국가에서만 접근 허용 | 해당 국가 IP 프록시 사용 |\n",
    "| 웹사이트 유저 추적 회피 | IP 숨기기 가능 |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a311bd",
   "metadata": {},
   "source": [
    "#  프록시(Proxy) 사이트 종류\n",
    "\n",
    "프록시는 목적과 사용 방식에 따라 여러 종류로 나눌 수 있어요.  \n",
    "크롤링에서 특히 많이 쓰이는 **프록시 서버 종류 + 추천 사이트**\n",
    "\n",
    "---\n",
    "\n",
    "#  1. **무료 프록시(Free Proxy)**  \n",
    "아무나 무료로 사용하는 프록시 서버.\n",
    "\n",
    "### ✔ 장점\n",
    "- 무료\n",
    "\n",
    "### ✔ 단점  \n",
    "- 속도 느림  \n",
    "- 금방 죽음(응답 없음)  \n",
    "- 보안 취약  \n",
    "- 신뢰성 낮음  \n",
    "- 크롤링에 거의 부적합\n",
    "\n",
    "### ✔ 무료 프록시 사이트 예  \n",
    "- FreeProxyList (https://free-proxy-list.net)  \n",
    "- ProxyScrape (https://proxyscrape.com/free-proxy-list)  \n",
    "- Spys.one  \n",
    "- Geonode free list  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#  2. **프라이빗 프록시(Private Proxy)**  \n",
    "유료로 구매하는 개인 전용 IP  \n",
    "(너만 사용 → 빠르고 안정적)\n",
    "\n",
    "### ✔ 장점\n",
    "- 속도 매우 빠름  \n",
    "- 거의 차단 안 됨  \n",
    "- 안정적  \n",
    "- 크롤링 실무에 적합  \n",
    "\n",
    "### ✔ 단점  \n",
    "- 유료(보통 월 단위)\n",
    "\n",
    "### ✔ 추천 사이트  \n",
    "- Bright Data (전 세계 제일 큰 프록시 업체)  \n",
    "- ScraperAPI  \n",
    "- Oxylabs  \n",
    "- Smartproxy  \n",
    "- Webshare (가성비 좋음)\n",
    "\n",
    "---\n",
    "\n",
    "#  3. **회전 프록시(Rotating Proxy)**  \n",
    "요청할 때마다 자동으로 프록시 IP가 바뀜  \n",
    "→ 크롤링에 가장 많이 사용됨\n",
    "\n",
    "### ✔ 장점\n",
    "- IP 자동 로테이션  \n",
    "- 차단 위험 낮음  \n",
    "- 장시간 크롤링에 최적\n",
    "\n",
    "### ✔ 단점\n",
    "- 유료\n",
    "\n",
    "### ✔ 추천 사이트  \n",
    "- Bright Data Rotating Proxy  \n",
    "- Oxylabs Rotating Residential  \n",
    "- ScrapingBee (API 방식)  \n",
    "- Zyte Smart Proxy Manager (Scrapy 기업)\n",
    "\n",
    "---\n",
    "\n",
    "#  4. **레지덴셜 프록시(Residential Proxy)**  \n",
    "\"진짜 가정집에서 쓰는 IP\"를 제공  \n",
    "(크롤링 탐지 우회에 최강)\n",
    "\n",
    "### ✔ 장점\n",
    "- 웹사이트에서 실제 사람으로 인식  \n",
    "- 차단 거의 없음\n",
    "\n",
    "### ✔ 단점\n",
    "- 비쌈  \n",
    "- 대량 크롤링은 비용 부담\n",
    "\n",
    "### ✔ 추천 사이트  \n",
    "- Bright Data Residential  \n",
    "- Smartproxy Residential  \n",
    "- Oxylabs Residential\n",
    "\n",
    "---\n",
    "\n",
    "#  5. **데이터센터 프록시(Datacenter Proxy)**  \n",
    "데이터센터에서 제공하는 IP  \n",
    "(지속적으로 빠름)\n",
    "\n",
    "### ✔ 장점\n",
    "- 빠름  \n",
    "- 싸고 대량으로 사용 가능  \n",
    "\n",
    "### ✔ 단점\n",
    "- 레지덴셜보다 차단 확률 높음\n",
    "\n",
    "### ✔ 추천 사이트  \n",
    "- Webshare  \n",
    "- Smartproxy  \n",
    "- IPRoyal  \n",
    "- ProxyRack\n",
    "\n",
    "---\n",
    "\n",
    "#  종류별 핵심 요약\n",
    "\n",
    "| 종류 | 특징 | 크롤링 적합도 |\n",
    "|------|------|---------------|\n",
    "| 무료 프록시 | 불안정, 느림 | ★☆☆☆☆ (연습용) |\n",
    "| 프라이빗 프록시 | 개인 전용, 빠름 | ★★★★☆ |\n",
    "| 회전 프록시 | 요청마다 IP 변경 | ★★★★★ (최적) |\n",
    "| 레지덴셜 프록시 | 가정집 IP, 탐지 우회 최강 | ★★★★★ |\n",
    "| 데이터센터 프록시 | 빠르고 저렴 | ★★★★☆ |\n",
    "\n",
    "---\n",
    "\n",
    "#  크롤링 용으로는 어떤 프록시가 최고인가?  \n",
    " **회전 프록시 + 레지덴셜 IP 조합**  \n",
    "(가장 자연스럽고 차단되지 않음)\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
