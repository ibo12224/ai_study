{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb69da1",
   "metadata": {},
   "source": [
    "\n",
    "##  신경망 활성화 함수 (Activation Functions in Neural Networks)\n",
    "\n",
    "\n",
    "\n",
    "### 1. 로지스틱 함수 (Logistic Function)\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "### 2. 시그모이드 함수 (Sigmoid Function)\n",
    "\n",
    "로지스틱 함수와 동일한 형태이며, 신경망에서 자주 사용됩니다.\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{e^z}{e^z + 1}\n",
    "$$\n",
    "\n",
    "### 3. 소프트맥스 함수 (Softmax Function)\n",
    "\n",
    "시그모이드 함수는 이진 분류(Binary Classification)에 주로 사용되며, 시그모이드 함수를 다중 분류(Multi-class Classification)를 위해 **일반화(Generalization)**한 것이 소프트맥스 함수입니다.\n",
    "\n",
    "$$\n",
    "\\sigma(z_j) = \\frac{e^{z_j}}{\\sum_{i=1}^{K} e^{z_i}}\n",
    "$$\n",
    "\n",
    "* $z_j$: $K$개의 클래스 중 $j$번째 클래스에 대한 입력값 (가중치 합)\n",
    "* $K$: 전체 클래스의 개수\n",
    "* $\\sum_{i=1}^{K} e^{z_i}$: 모든 클래스에 대한 $\\exp(z)$ 값의 합 (정규화 항)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b0d9b",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tGSrq3hfKFBgKntB'>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
