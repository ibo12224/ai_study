{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eee5962",
   "metadata": {},
   "source": [
    "##  Odds ratio\n",
    "\n",
    "$C_1$과 $C_2$를 구분하는 **binary classification**에서 하나의 데이터 포인트 $x$가 주어졌을 때, 이것이 $C_1$으로 분류될 확률을 $y$라고 보면, 같은 데이터가 $C_2$로 분류될 확률은 $1-y$이다.\n",
    "\n",
    "$$P(C_1|x) = y$$\n",
    "$$P(C_2|x) = 1-y$$\n",
    "\n",
    "$$\n",
    "\\text{Choose} \\begin{cases} C_1 & \\text{if } y > 0.5 \\\\ C_2 & \\text{if } y \\le 0.5 \\end{cases}\n",
    "\\quad \\Longleftrightarrow \\quad \\frac{y}{1-y} > 1 \\quad (\\text{Odds})\n",
    "\\quad \\Longleftrightarrow \\quad \\log_e \\frac{y}{1-y} > 0 \\quad (\\text{Logit})\n",
    "$$\n",
    "\n",
    "다시 말해 $x$가 주어졌을 때, 결과값 $y$가 $0.5$보다 크면 $x$는 $C_1$ 클래스이고, $y$가 $0.5$보다 작으면 $x$는 $C_2$ 클래스일 것이라고 볼 수 있다. 이를 간략하게 표현한 것이 **Odds**라는 단어이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d96236",
   "metadata": {},
   "source": [
    "## Odds\n",
    "\n",
    "Odds(승산)는 Probability의 또 다른 표현법으로 **실패 비율 대비 성공 비율**을 이야기한다. 다시 말해, **성공 확률이 실패 확률에 비해 얼마나 더 큰가**이다. 즉, Odds가 클수록 성공 확률이 크다는 의미이다.\n",
    "\n",
    "Odds는 도박에서 얻을 확률(**Pay off**)과 잃을 확률(**Stake**)의 비율을 뜻하는 영어 단어이다. $C_1$일 확률을 '얻는다'로 보고, $C_2$일 확률을 '잃는다'고 본다. 이때 $y$가 $1-y$보다 커서 비율이 $1$보다 커질 때는 $C_1$ 클래스로 분류하고, 그 반대는 $C_2$ 클래스로 분류한다는 뜻이다.\n",
    "\n",
    "만약 어떤 이벤트가 일어날 확률이 $60\\%$라고 할 때, Odds는 $0.6/0.4 = 1.5$와 같이 계산된다.\n",
    "\n",
    "따라서 임의의 사건 $A$가 발생할 확률 $p$에 대한 Odds는 아래와 같이 정의된다.\n",
    "\n",
    "$$\n",
    "\\text{odds} = \\frac{P(A)}{P(A^c)} = \\frac{p}{1-p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dbefab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c9da4",
   "metadata": {},
   "source": [
    "## Logit\n",
    "\n",
    "Logit은 Odds에 자연로그를 씌운 것으로 아래와 같이 정의된다. Logit + Odds = $\\text{logistic} + \\text{probit} = \\text{logit}$ 에서 나온 말이다. Log 변환은 통계학에서 자주 사용하는 변환이다.\n",
    "\n",
    "$$\n",
    "L = \\log(\\text{Odds}) = \\ln \\frac{p}{1-p}\n",
    "$$\n",
    "\n",
    "Odds는 그 값이 $1$보다 큰지가 결정의 기준이고, Logit은 $0$보다 큰지가 결정의 기준이다. 이러한 **Logit 함수**와 **Sigmoid(logistic) 함수**는 서로 **역함수($y=x$에 대하여 대칭)** 관계이다.\n",
    "\n",
    "$$\n",
    "p = \\frac{1}{1+e^{-L}} = \\frac{e^{L}}{e^{L}+1}\n",
    "$$\n",
    "\n",
    "logit 함수에서 sigmoid 함수를 유도해보면 아래와 같다.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Logit}(p) &= \\log_{e} \\left( \\frac{1}{1-p} \\right) = L \\\\\n",
    "\\frac{p}{1-p} &= \\exp(L) \\\\\n",
    "\\frac{1}{p} - 1 &= \\frac{1}{\\exp(L)} \\\\\n",
    "\\frac{1}{p} &= \\frac{1+\\exp(L)}{\\exp(L)} \\\\\n",
    "&= \\frac{\\exp(L)+\\exp(L)}{\\exp(L)} \\\\\n",
    "p &= \\frac{\\exp(L)}{1+\\exp(L)} \\\\\n",
    "&= \\frac{1}{1+\\exp(-L)} \\\\\n",
    "&= \\text{Sigmoid}(L)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\\therefore \\text{Logit}(p)=L, \\text{Sigmoid}(L)=p$$\n",
    "\n",
    "결국 $\\text{logit}(y) = t$, $\\text{sigmoid}(t) = y$로 다시 표현할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8e53df",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba687c7c",
   "metadata": {},
   "source": [
    "##  로지스틱 회귀는 왜 시그모이드 함수를 사용하는가?\n",
    "\n",
    "Binary classification(이진 분류)에서 확률 $p$의 범위는 $[0, 1]$이다. 따라서 단순 선형 함수 $y=Wx+b$로 풀기 힘들다는 것은 이전에 언급한 바가 있다. 그래서 우리는 **Odds**와 **Logit**을 이용해야 한다.\n",
    "\n",
    "### Logit 변환의 필요성\n",
    "\n",
    "* Odds의 범위는 $[0, \\infty]$이고, $\\log(\\text{Odds}(p))$는 $[-\\infty, \\infty]$ 범위로 넓어진다. 즉, Logit을 사용하면 값이 **실수 전체**가 된다.\n",
    "* 이는 단순 선형 함수로 풀기 힘든 binary classification 문제를 $\\log(\\text{Odds}(p)) = Wx+b$로 선형 회귀 분석할 수 있도록 만들어준다.\n",
    "\n",
    "$$\\log(\\text{Odds}(p)) = Wx+b \\quad \\rightarrow \\quad \\log \\left( \\frac{p}{1-p} \\right) = Wx+b$$\n",
    "\n",
    "### 시그모이드 함수의 유도\n",
    "\n",
    "이 식을 $p$에 대해 정리하면 아래의 수식, 즉 **시그모이드(Sigmoid) 함수**가 나온다.\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{1+e^{-(Wx+b)}}\n",
    "$$\n",
    "\n",
    "$x$ 데이터가 주어졌을 때 성공 확률을 예측하는 **Logistic Regression(로지스틱 회귀)**은 결국 Sigmoid 함수의 $W$와 $b$를 찾는 문제가 된다.\n",
    "\n",
    "### 다중 클래스 분류와의 연결\n",
    "\n",
    "또한, 시그모이드 함수는 $[0, 1]$ 범위인 확률을 $[-\\infty, \\infty]$로 넓히기 때문에 보통 멀티 클래스 분류 문제에서 **softmax 함수**의 입력으로 사용된다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
